docs: |
  SPACK on BLUECRAB: software specification
  Build and organize software for the *Blue Crab* cluster.
  USAGE: 
    make spack_hpc_run run=specs/spack_hpc_run.yaml \
    deploy=specs/spack_bc_proot.yaml name=bc-prelim live
  See `specs/bluecrab.yaml` for the easy interface.

# settings: spack source and environments locations 
# config.json stores spot as spack and spot_envs as spack_envs
spot:       ./local/spack
spot_envs:  ./local/envs-spack

history:
  general: |
    Spack on Blue Crab requires a few files for configuration:
      - this file, for telling spack what to install
      - bluecrab.yaml uses a menu system to kick off the builds
      - spack_bc_run.yaml selects hardware
      - decoy environments in spack_bc_proot.yaml and spack_bc_singularity.yaml
    The build sequence follows this order:
      1. Set up a decoy location. 
      2. Use `specs/bluecrab/yaml` to run the prelim and compile a compiler.
      3. Use stage01 and the singleton recipe to build packages sequentially.
      4. Switch to a proot environment to build openmpi and downstream.
      5. Use the "test" feature in bluecrab.yaml to test.
      6. Preview the spack concretize result with the "live" feature.
      7. Push to production after testing with `bluecrab.yaml` rsync command.
  series5: |
    # this is the most recent build
    # set up a new decoy site
    mkdir -p /exec/rbradley/buildsite/tz4
    # update with the new decoy site
    vim specs/spack_bc_singularity.yaml
    # merge the latest spack packages into our custom branch
    cd local/spack
    git checkout develop
    git pull
    git checkout bluecrab
    git merge develop
    # start the build
    echo -e "build\n" | menu_name=bc-prelim make do specs/bluecrab.yaml
    echo -e "build\n" | menu_name=bc-gcc-7-compiler make do specs/bluecrab.yaml
    # [streamline 890f6b7] spack series five python build
    menu_name=bc-singleton make do specs/bluecrab.yaml # live
    # python compiles in 56min on /dev/shm
    # using /dev/shm for now for a few builds
    # note that we force py-tensorflow there because bazel cannot use NFS
    # [streamline 611edcd] building software on bluecrab
    # added tensorflow to the bc-singleton recipe
    echo -e "live\n" | menu_name=bc-singleton make do specs/bluecrab.yaml
    # currently building up singleton with python then py-tensorflow
    # [streamline 95e44c9] tensorflow build
    # failures on tensorflow with mysterious excess_args issue
    # next: attempt proot with /dev/shm from gpudev002
    spack mirror create py-tensorflow+cuda cuda_arch=35

###
### SOFTWARE SPECIFICATION
###

arch: &arch "arch=linux-centos7-haswell"
gcc-back-compiler: &gcc-back-compiler gcc@4.8.5
gcc-back: &gcc-back !strflush ["%", *gcc-back-compiler]
gcc-7-compiler: &gcc-7-compiler gcc@7.4.0
gcc-7: &gcc-7 !strflush ["%", *gcc-7-compiler, " ", *arch]
python3: &python-3 >-
  python@3.7.6 +bz2 +ctypes +dbm +debug 
  +libxml2 +lzma +optimizations +pic +pyexpat +pythoncmd 
  +readline +shared +sqlite3 +ssl +uuid +zlib
python36: &python-36 >-
  python@3.6.8 +bz2 +ctypes +dbm +debug 
  +libxml2 +lzma +optimizations +pic +pyexpat +pythoncmd 
  +readline +shared +sqlite3 +ssl +uuid +zlib
r36: &r36 !chain
  - r@3.6.1 +memory_profiling
  - *python-3

wl02: &wl02 !merge_lists
  #! bug: define this here because !merge_lists cannot use refs
  - &r-packages [r-devtools, r-rcpp]
  - &wl01 [gcc, python, r]
  # each target R package must include dependencies in the whitelist
  - &wl-r-devtools [
    r-callr, r-processx, r-ps, r-r6, r-cli, r-assertthat, r-crayon, 
    r-digest, r-git2r, r-httr, r-curl, r-jsonlite, r-mime, r-openssl, 
    r-askpass, r-sys, r-memoise, r-pkgbuild, r-desc, r-rprojroot, 
    r-backports, r-prettyunits, r-magrittr, r-withr, r-pkgload, 
    r-rlang, r-rstudioapi, r-rcmdcheck, r-sessioninfo, r-xopen, 
    r-remotes, r-roxygen2, r-brew, r-commonmark, r-purrr, r-rcpp, 
    r-stringi, r-stringr, r-glue, r-xml2, r-testthat, r-evaluate, 
    r-praise, r-usethis, r-clipr, r-clisymbols, r-fs, r-gh, r-ini, 
    r-whisker, r-yaml]
  # singleton non-MPI packages
  - &base-singletons [
      cmake, fontconfig, freetype] 
  # python-dependent codes
  - &python-deps [cairo]
  # MPI-compiled codes
  - [gromacs, lammps]
  # incidental to MPI-compiled codes
  - [netcdf-c, hdf5]
  # tensorflow
  - [py-tensorflow@2.1.0 ^cuda@10.1.243]
  #! building up the tensorflow requirements
  - [python, cuda, cudnn, py-absl-py, py-setuptools, py-six, py-astor, 
    py-gast, py-google-pasta, py-grpcio, py-cython, py-keras-applications, 
    py-keras-preprocessing, py-numpy, py-opt-einsum, py-protobuf, py-scipy, 
    py-pybind11, py-termcolor, py-wheel, py-wrapt, py-pip]
  #! previous whitelist for py-tensorflow
  - [py-absl-py, py-setuptools, py-six, py-astor, py-gast,
    py-google-pasta, py-grpcio, py-cython, py-keras-applications,
    py-keras-preprocessing, py-numpy, py-opt-einsum, py-protobuf,
    py-scipy, py-pybind11, py-termcolor, py-wheel, py-wrapt]
  #! more tensorflow requirements
  #! bug: whitelist is difficult to properly infer so
  #!   consider disabling blacklist_implicits 
  - [intel-mkl, libxml]

openmpi-3-1: &openmpi-3-1 >-
  openmpi@3.1.5 %gcc@7.4.0 arch=linux-centos7-haswell
  +cuda +cxx_exceptions fabrics=verbs 
  +legacylaunchers +pmi +vt schedulers=slurm 
  ^slurm@17.11.12 

fftw38: &fftw38 >-
  fftw@3.3.8 +mpi +openmp ~pfft_patches precision=double,float 

lammps: &lammps-v01 >-
  lammps@20190807 +cuda cuda_arch=35
  +asphere +body +class2 +colloid +compress +coreshell
  +dipole +exceptions +ffmpeg +granular +jpeg +kokkos +kspace ~latte +lib 
  +manybody +mc +meam +misc +molecule +mpi +mpiio +openmp +peri +png +poems 
  +python +qeq +reax +replica +rigid +shock +snap +srd +user-atc +user-h5md 
  +user-lb +user-misc +user-netcdf +user-omp +user-reaxc +voronoi
  +user-diffraction 

netcdf-c: &netcdf-c-v01 >-
  netcdf-c@4.7.3 %gcc@7.4.0 ~dap ~hdf4 maxdims=1024 maxvars=8192 
  +mpi ~parallel-netcdf +pic +shared

hdf5: &hdf5-v01 >-
  hdf5@1.10.6 ~cxx ~debug ~fortran +hl +mpi +pic +shared +szip ~threadsafe

cudnn-v01: &cudnn-v01 !str [cudnn@7.6.5.32-10.2-linux-x64, *gcc-7]
cudnn-v03: &cudnn-v03 !str [cudnn@7.6.4.38-10.1-linux-x64, *gcc-7]
cuda-10-2: &cuda-10-2 !str [cuda@10.2.89, *gcc-7]
cuda-10-1: &cuda-10-1 !str [cuda@10.1.243, *gcc-7]
cuda-10-0: &cuda-10-0 !str [cuda@10.0.130, *gcc-7]
mkl-v01: &mkl-v01 !str [intel-mlk@2020.0.166 +shared threads=tbb, *gcc-7]
scipy-v01: &scipy-v01 !str [py-scipy ^intel-mkl, *gcc-7]
tf-est-v02: &tf-est-v02 !str [py-tensorflow-estimator@2.0.0, *gcc-7]

tf-v01: &tf-v01 !chain
  - !str 
    - py-tensorflow@2.1.0 +cuda cuda_arch=35 +numa ~mpi +mkl ~tensorrt ~verbs
    - *gcc-7
  - *cuda-10-2
  - *cudnn-v01
  - *scipy-v01
  - *python-3

tf-v02: &tf-v02 !chain
  - !str 
    - py-tensorflow@2.0.0 +cuda cuda_arch=35 +numa ~mpi +mkl ~tensorrt ~verbs
    - *gcc-7
  - *cuda-10-2
  - *cudnn-v01
  - *python-3

tf-v03: &tf-v03 !chain
  - !str 
    - py-tensorflow@2.1.0 +cuda cuda_arch=35 +numa ~mpi +mkl ~tensorrt ~verbs
    - *gcc-7
  - *cuda-10-1
  - *cudnn-v03
  - *python-3

tf-v04: &tf-v04 !chain
  - !str 
    - py-tensorflow@2.1.0 +cuda cuda_arch=35 +numa ~mpi ~mkl ~tensorrt ~verbs
    - *gcc-7
  - *cuda-10-2
  - *cudnn-v01
  - *scipy-v01
  - *python-3

tf-v05: &tf-v05 !chain
  - !str 
    - py-tensorflow@2.1.0 +cuda cuda_arch=35 +numa ~mpi ~mkl ~tensorrt ~verbs
    - *gcc-7
  - *cuda-10-2
  - *cudnn-v01
  - *scipy-v01
  - *python-36

###
### CONFIGURATION
###

# spack environments template (spack.yaml)
template_basic: 
  concretization: separately
  mirrors: {}
  repos: []
  upstreams: {}
  modules:
    enable: []
  definitions: []
  packages: {}
  config: 
    checksum: false
  specs: []
  view: false

# BUILD CONFIGURATION
# matched to the deploy file
bc-config: 
  config: &bc-config
    install_tree: /software/apps/spack/a02
    module_roots: 
      lmod: &module-spot /software/apps/spack/m02
    build_stage:
      - "$TMPDIR/$USER/spack-stage"
    misc_cache: /software/apps/spack/a02/cache_misc
  packages: &packages
    slurm:
      paths:
        slurm@17.11.12: /software/apps/slurm/17.11.12.1.marcc
      buildable: false
      version: []
      providers: {}
      modules: {}
      compiler: []
    openssl:
      paths: {openssl@1.0.2k: /usr}
      buildable: false
    curl:
      # r-curl issue ca 2020.02.08
      version: [7.63.0]

bc-config-modules:
  s01: &mod-stage-01
    enable: 
      - lmod
    lmod:
      all:
        conflict: []
        environment:
          unset: []
        filter:
          environment_blacklist: []
        load: []
      blacklist_implicits: false
      core_compilers:
      - *gcc-back-compiler
      hash_length: 0
      naming_scheme: '{name}/{version}/{compiler.name}/{compiler.version}'
      hierarchy:
      - compiler
      verbose: false

bc-config-modules:
  s01: &mod-stage-02
    enable: 
      - lmod
    lmod: &mod-stage-02-lmod
      all:
        conflict: []
        environment:
          unset: []
        filter:
          environment_blacklist: []
        load: []
      blacklist_implicits: true
      core_compilers:
      - *gcc-back-compiler
      blacklist: 
      - *gcc-back
      hash_length: 4
      hierarchy:
      - compiler
      - mpi
      verbose: false
      openmpi:
        environment:
          set:
            OMPI_MCA_mpi_warn_on_fork: '0'
          unset: []
        filter:
          environment_blacklist: []
        load: []
        conflict: []

mods_config:
  notes: |
    To add a module with python or R dependencies:
      1. Get the full text of the concretize output.
      2. Extract all of the py-name or r-name packages.
      3. Add these to the whitelist above.
      4. Add `autoload: all` for the target under the whitelist below.
    Dev note: it would be nice if this was more seamless.
  mods: &mc01
    config: *bc-config
    packages: *packages
    modules: *mod-stage-01
  mods: &mc02
    config: *bc-config
    packages: *packages
    modules: 
      enable: [lmod]
      lmod: 
        << : *mod-stage-02-lmod
        whitelist: *wl02
        python: {autoload: all}
        py-tensorflow: 
          autoload: all
          suffixes:
            "^cuda@10.2.89": "cuda-10.2"
            "^python@3.7.6": "py37"
        py-scipy: {autoload: all}
        py-numpy: {autoload: all}

###
### ENVIRONMENTS
###

bc-prelim: 
  notes: Initial test before running a compiler build.
  envs:
    - find_compilers: null
    - check_compiler: *gcc-back-compiler
    - bootstrap: null
    - name: env-prelim
      via: template_basic
      mods: *mc01
      specs:
        - !str [zlib, *gcc-back, *arch]
        - !str [cmake, *gcc-back, *arch]

bc-gcc-7-compiler:
  notes: Compile gcc 7 for Blue Crab.
  envs:
    - find_compilers: null
    - check_compiler: *gcc-back-compiler
    - bootstrap: null
    - name: &env-gcc-7-compiler env-gcc-7-compiler
      via: template_basic
      mods: *mc01
      specs:
        - !str &gcc-7-out [*gcc-7-compiler, *gcc-back, *arch]
    - find: *gcc-7-compiler 
      name: *env-gcc-7-compiler

bc-stage01:
  notes: 
    Blue Crab base apps built on gcc7.
    This list was build sequentially during testing.
    See Ryan's notes up to 2020.02.08 for details.
    Note a failed attempt to use spack to supply deps for pdftools namely poppler.
    This failure was due to differences between spack poppler and target poppler-cpp.
  envs:
    - check_compiler: *gcc-7-compiler
    - name: &env-stage01 env-stage01
      via: template_basic
      mods: *mc02
      specs: &stage01-specs !merge_lists
        - [*gcc-7-out]
        # base python
        - [!str [*python-3, *gcc-7]]
        - [!strflush [py-pip, " ^", *python-3, *gcc-7]]
        # base R 
        - [!str [*r36, *gcc-7]]
        # large set of R packages
        - !loopcat
          base: !strflush [" ^", *r36, *gcc-7]
          loop: *r-packages
        # singletons
        - !loopcat
          base: *gcc-7
          loop: *base-singletons
        # tensorflow
        - - !chain [*scipy-v01, *python-3]
          - *tf-v01
        # extra standalone cuda
        - [*cuda-10-1, *cuda-10-0]
        # gdb with python
        - [!strflush ['gdb+python', " ^", *python-3, *gcc-7]]

bc-stage02:
  notes: |
    Build OpenMPI and MPI-dependent applications.
    Note that we removed lmod refresh after making the production recipe.
    Build strategy: build openmpi and dependencies with proot on debug
    You must have libibverbs and MPI must work. Singularity is not enough.
    There is a curl fork bomb when using proot so use mirror.
    Use `spack mirror create openblas` to fetch before building live in proot.
    Currently we attach CUDA to MPI for CUDA-aware MPI apps.
    Lammps below requires explicit DPYTHON_EXECUTABLE when building in proot.
  envs:
    - check_compiler: *gcc-7-compiler
    - name: &env-stage02 env-stage02
      via: template_basic
      mods: *mc02
      specs: &stage02-specs !merge_lists
        - *stage01-specs
        # openmpi builds here with cuda which can change for downstream
        # WARNING: build openmpi in proot
        - [!chain [*openmpi-3-1, &cuda9 "cuda@9.2.88"]]
        - !loopcat
          # openmpi carries cuda so we omit it from the loop
          base: !strflush [" ^", *openmpi-3-1, "^ ", *cuda9]
          loop: 
            - !str [*fftw38, *gcc-7]
            - !chain
              - !str ["gromacs@2019.2 +cuda simd=AVX2_256", *gcc-7]
              - *fftw38
            - *hdf5-v01
            - !chain
              - *netcdf-c-v01
              - *hdf5-v01
            - !chain
              # note that we had to patch spack to set the right python
              # when building in proot via: DPYTHON_EXECUTABLE
              - !str [*lammps-v01, *gcc-7]
              - *python-3
              - *hdf5-v01
              - *netcdf-c-v01
              - *fftw38

bc-singleton:
  notes: |
    Singleton stage for quick building and less concretization.
    Use this for testing things before you add them to the stages
  envs:
    - check_compiler: *gcc-7-compiler
    - name: &env-singleton env-singleton
      via: template_basic
      mods: *mc02
      specs: 
      specs: !merge_lists
        - [*gcc-7-out]
        # base python
        - [!str [*python-36, *gcc-7]]
        - [!strflush [py-pip, " ^", *python-36, *gcc-7]]
        # tensorflow
        - - *tf-v05
        #! adding broken python to test hierarchy
        - [!str [*python-3, *gcc-7]]
        - [!strflush [py-pip, " ^", *python-3, *gcc-7]]
        - - *tf-v01
    # modules for testing mode
    - lmod_refresh: null
      name: *env-singleton

###
### EXTRA MODULEFILES
### 

# anaconda modulefile
anaconda-modulefile: &anaconda-modulefile |
  local name = myModuleName()
  local version = myModuleVersion()
  local root = pathJoin(marccRoot(), name, version)
  help([[ 

  Anaconda environments.

  This module provides the Anaconda package manager which allows
  users to install and user their own virtual environments from 
  the "conda" package distribution repositories.

  To prepare a custom environment:

  module load anaconda
  conda env create -p ./path/to/env
  conda activate ./path/to/env

  You only create the environment once, but you must activate it whenever
  you want to use it. You can also deactivate it ("conda deactiate").

  Note that you will use the absolute path to access this environment later.
  Please put anaconda environments in ~/ or ~/data (because the Lustre 
  filesystem on  ~/scratch and ~/work is not optimized for executables).

  More documentation is here: https://docs.conda.io/en/latest/

  MARCC provides custom environments to save you time (and disk space).
  See the available environments

  See available prepared environments with "conda env list".
  Load these environments with "conda activate <name>". 

  ]])
  whatis("Sets up environment for "..name.." version "..version)
  local conda_exe = os.getenv("CONDA_EXE")
  local myShell = myShellName()
  if (conda_exe == nil or conda_exe == "") or mode()=="load" then
    if (myShell == "bash") then
        cmd = "source " .. pathJoin(root,"/etc/profile.d/conda.sh")
    else
        cmd = "source " .. pathJoin(root,"/etc/profile.d/conda.csh")
    end
    execute{cmd=cmd, modeA = {"load"}}
  elseif mode()=="unload" then
    if (myShell == "bash") then
      cmd = "conda deactivate || : ; " ..
        "unset ANACONDA3ROOT; unset PYTHONROOT; unset CONDA_EXE; " ..
        "unset CONDA_PYTHON_EXE; unset _CE_CONDA; unset _CE_M; " ..
        "unset __add_sys_prefix_to_path; unset __conda_activate; " ..
        "unset __conda_reactivate; unset __conda_hashr; unset CONDA_SHLVL; " ..
        "unset conda"
    else
      cmd = "conda deactivate; " ..
        "unsetenv CONDA_EXE; unsetenv _CONDA_ROOT; unsetenv _CONDA_EXE; " ..
        "unsetenv CONDA_PYTHON_EXE; unset CONDA_SHLVL; unalias conda"
    end
    execute{cmd=cmd, modeA = {"unload"}}
    -- you must remove conda from the path after deactivating
    remove_path("PATH",pathJoin(root,"condabin"))
  end

# marcc module
marcc-modulefile: &marcc-modulefile |
  append_path("PATH","/software/apps/marcc/bin")
  append_path("PATH","/software/apps/slurm/current/bin")
  append_path("PATH","/software/apps/slurm/current/sbin")
  append_path("MANPATH","/software/apps/slurm/current/share/man")
  append_path("LD_LIBRARY_PATH","/software/apps/slurm/current/lib")
  append_path("LD_LIBRARY_PATH","/software/apps/slurm/current/lib/slurm")
  append_path("LIBRARY_PATH","/software/apps/slurm/current/lib")
  append_path("LIBRARY_PATH","/software/apps/slurm/current/lib/slurm")

# extensions module for the centos7 repo
extensions-modulefile: &extensions-modulefile !strflush
  - '-- extension modulefile for software that requires /software/centos7'
  - "\n"
  - '-- append paths so spack can do the heavy lifting'
  - "\n"
  - 'append_path("LD_LIBRARY_PATH","/software/centos7/lib64:'
  - '/software/centos7/lib:'
  - '/software/centos7/usr/lib64:/software/centos7/usr/lib")'
  - "\n"
  - 'append_path("LIBRARY_PATH","/software/centos7/lib64:'
  - '/software/centos7/lib:/software/centos7/usr/lib64:'
  - '/software/centos7/usr/lib")'

# matlab modulefile from the original software tree
matlab-modulefile: &matlab-modulefile |
  whatis([[ Matlab: adds MATLAB to your environment variables ]])
  local name  = myModuleName() -- software name
  local version = myModuleVersion() -- should be of form MAJ.MIN.PATCH
  local root = pathJoin(marccRoot(), name, version, "bin")
  prepend_path("PATH", pathJoin(root, " "))
  setenv("LM_LICENSE_FILE","27000@172.16.0.10")
  -- fonts and X11 XKB are here
  setenv("FONTCONFIG_PATH","/software/centos7/etc/fonts/")
  setenv("XKB_CONFIG_ROOT","/software/centos7/usr/share/X11/xkb/")
  local function isempty(s)
    return s == nil or s == ''
  end
  -- no proot if using non-interactive job
  if not isempty(os.getenv("SLURM_PTY_WIN_ROW")) or
    isempty(os.getenv("SLURM_JOBID")) then
    local bashStr = "/software/apps/proot/5.1.0/bin/proot -b " ..
      "/software/centos7/usr/:/usr/ -b $HOME/.matlab/:/tmp/ " ..
      "/software/apps/matlab/R2019a/bin/matlab -softwareopengl \"$@\"'
    local cshStr = "/software/apps/proot/5.1.0/bin/proot -b " ..
      "/software/centos7/usr/:/usr/ -b $HOME/.matlab/:/tmp/ " .. "
      "/software/apps/matlab/R2019a/bin/matlab -softwareopengl $* "
    set_shell_function("matlab", bashStr, cshStr)
  end

###
### PRODUCTION DEPLOYMENT
### 

production:
  notes: |
    Complete spack build for production.
    Remember to rsync this to production.
    This recipe is best for finalizing modules.
    We recommend adding software in the stages above.
  envs:
    - check_compiler: *gcc-7-compiler
    - name: &env-production env-production
      via: template_basic
      mods: *mc02
      specs: *stage02-specs
    - lmod_refresh: null
      name: *env-production
    # we keep the nested openmpi with hashes for clarity
    # otherwise use: lmod_hide_nested: *module-spot
    # removing hashes failed via: lmod_remove_nested_hashes
    - lmod_hooks:
      # use of the original module facilitates loading via `ml -stack`
      - moduleroot: *module-spot
        modulefile: linux-centos7-x86_64/Core/original.lua
        contents: !strflush
          - 'execute{cmd="echo [STATUS] Loading the original modules. '
          - 'If your default modules includes stack you must clear your cache. '
          - '&& export MODULEPATH='
          - '/software/lmod/modulefiles/compiler_and_base:'
          - '/software/lmod/modulefiles/apps && '
          # return to system defaults. see note below on stack modulefile
          - 'export LMOD_SYSTEM_DEFAULT_MODULES=MARCC/summer-2018 && '
          - 'ml restore",modeA={"load"}}'
      - mkdir: !strflush [*module-spot, '/linux-centos7-x86_64/Core/stack']
      - moduleroot: *module-spot
        modulefile: linux-centos7-x86_64/Core/stack/0.4.lua
        contents: !strflush 
          - 'help([[You are currently using the stack module. '
          - 'This module provides a new set of software managed by Spack. '
          - 'To return to the original modules use ml original.]])'
          - "\n"
          - 'execute{cmd="echo [STATUS] Using the stack module. '
          - 'To use original modules run ml original.",modeA={"load"}}'
          - "\n"
          - 'execute{cmd="echo [STATUS] Using the stack module. '
          - 'To use original modules run ml original.",modeA={"unload"}}'
      # the following modulefile connects to the original modules
      # note that this exists outside the test environment
      - mkdir: /software/lmod/modulefiles/apps/stack
      - moduleroot: /software/lmod/modulefiles/apps/stack
        modulefile: 0.4.lua
        contents: !strflush
          - 'help([[The "stack" module loads the new MARCC software modules. '
          - 'These modules are generated by Spack.]])'
          - "\n"
          - 'execute{cmd="echo [STATUS] Loading the stack module. '
          - 'To use original modules run ml original. '
          - '&& export MODULEPATH='
          - '/software/apps/spack/m02/linux-centos7-x86_64/Core && '
          # setting stack as default prevents conflicts on subshell caused
          #   by the /etc/profile.d/z00_lmod.sh on blue crab however users
          #   will still have to reload some of their modules
          #! dev note: should you have to reload on interact? should you inherit?
          - 'export LMOD_SYSTEM_DEFAULT_MODULES=stack/0.4 && '
          - 'ml purge && ml stack/0.4 && ml gcc/7.4.0 && ml openmpi",modeA={"load"}}'
          - "\n"
          - 'execute{cmd="echo [STATUS] Using the stack module. '
          - 'To use original modules run ml original.",modeA={"unload"}}'
      - mkdir: !strflush [*module-spot, 
        '/linux-centos7-x86_64/Core/marcc']
      - moduleroot: !strflush [*module-spot, 
        '/linux-centos7-x86_64/Core/marcc']
        modulefile: 2020.02.lua
        # append paths to avoid slurm interference
        contents: *marcc-modulefile
    - lmod_defaults:
      - !strflush 
        - *module-spot
        - '/linux-centos7-x86_64/gcc/7.4.0/openmpi/3.1.5.lua'
      - !strflush [*module-spot,'/linux-centos7-x86_64/Core/gcc/7.4.0.lua']
    # external modules from the original module tree
    - lmod_hooks:
      - mkdir: !strflush [*module-spot, 
        '/linux-centos7-x86_64/Core/anaconda']
      - moduleroot: !strflush [*module-spot,
        '/linux-centos7-x86_64/Core/anaconda']
        modulefile: 2019.03.lua
        contents: *anaconda-modulefile
      - mkdir: !strflush [*module-spot,
        '/linux-centos7-x86_64/Core/extensions']
      - moduleroot: !strflush [*module-spot,
        '/linux-centos7-x86_64/Core/extensions']
        modulefile: 2020.02.lua
        contents: *extensions-modulefile
      - mkdir: !strflush [*module-spot, 
        '/linux-centos7-x86_64/Core/matlab']
      - moduleroot: !strflush [*module-spot, 
        '/linux-centos7-x86_64/Core/matlab']
        modulefile: R2019a.lua
        contents: *matlab-modulefile
