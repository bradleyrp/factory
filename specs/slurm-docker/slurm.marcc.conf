AccountingStorageEnforce=associations,limits,qos,safe
AccountingStorageHost=bc-slurmdbd
AccountingStorageTRES=cpu,mem,gres/gpu
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageUser=slurm
AccountingStoreJobComment=YES
AuthInfo=socket=/var/run/munge/munge.socket.2
AuthType=auth/munge
CheckpointType=checkpoint/none
ClusterName=marcc
CompleteWait=32
ControlAddr=bc-mgmt
ControlMachine=bc-mgmt
CryptoType=crypto/munge
DefaultStorageHost=bc-slurmdbd
DisableRootJobs=NO
EnforcePartLimits=YES
FastSchedule=1
GresTypes=gpu
HealthCheckInterval=300
# HealthCheckProgram=/software/apps/slurm/marcc-slurm-plugins/nhc_marcc
JobAcctGatherType=jobacct_gather/cgroup
JobAcctGatherFrequency=30
JobCompHost=bc-slurmdbd
KillOnBadExit=1
# MailProg=/software/apps/slurm/current/bin/smail
MaxArraySize=50000
MaxJobCount=250000
MaxMemPerCPU=5120
MessageTimeout=90 
MinJobAge=301
MpiDefault=none
PreemptType=preempt/none
PriorityDecayHalfLife=0-0
PriorityMaxAge=7-0
PriorityCalcPeriod=5
PriorityFlags=MAX_TRES,FAIR_TREE 
PriorityUsageResetPeriod=none
PriorityType=priority/multifactor
PriorityWeightAge=300000
PriorityWeightFairshare=100000
PriorityWeightJobSize=10000
PriorityWeightPartition=10000
PriorityWeightQOS=10000
ProctrackType=proctrack/cgroup
PropagatePrioProcess=2
RebootProgram="/bin/ipmitool chassis power reset"
ReturnToService=2
SchedulerParameters=kill_invalid_depend,assoc_limit_continue,default_queue_depth=100,nohold_on_prolog_fail,sched_min_interval=2000000,bf_yield_interval=1000000,bf_continue,max_rpc_cnt=450,bf_max_job_test=30000,bf_interval=600,bf_window=6060,pack_serial_at_end,bf_max_job_user=1000
SchedulerType=sched/backfill
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory
SlurmUser=slurm
# SlurmctldLogFile=/home/slurm/slurmctld
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmctldPidFile=/var/run/slurmd/slurmctld.pid
SlurmdPidFile=/var/run/slurmd/slurmd.pid
# SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmctldTimeout=900
SlurmdLogFile=/var/log/slurm/slurmd
# SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm/
SlurmdTimeout=3600
# SlurmSchedLogFile=/home/slurm/slurmschedlog
# SlurmSchedLogLevel=1
# StateSaveLocation=/var/lib/slurm/slurmstate/statesave
StateSaveLocation=/var/lib/slurmd
TaskPlugin=task/affinity,task/cgroup
TaskPluginParam=Sched
TCPTimeout=20
TopologyPlugin=topology/none
UnkillableStepTimeout=180
#CliFilterPlugins=lua
JobSubmitPlugins=lua
PropagateResourceLimitsExcept=MEMLOCK
#
# MEMORY LIMITS
# Note that RealMemory in this section is physical memory
# We set MemSpecLimit to 10240MB to reserve space on each node.
# The docs (https://slurm.schedmd.com/slurm.conf.html) say the MemSpecLimit is 
# the "Amount of memory, in megabytes, reserved for system use and not available 
# for user allocations." This requires a *_Memory option in SelectType to ensure 
# the memory is consumable. See "default memory settings" below for more detail.
# N.b. this setting is necessary for enforcing a cluster-wide system memory 
# reservation so that we can set a *highest* RAM/CPU value (see below) without 
# oversubscribing memory on the nodes with less than that.
#MemSpecLimit=10400
#
# NOTE: MemSpecLimit won't work, so this is implemented such that RealMemory=(available memory in GB) - 10GB
NodeName=compute[0001-0676] Feature=haswell weight=30 Sockets=2 CoresPerSocket=12 RealMemory=118000 State=UNKNOWN
NodeName=bigmem[0001-0050] Feature=ivybridge,lrgmem weight=120 Sockets=4 CoresPerSocket=12 RealMemory=1014000 State=UNKNOWN
NodeName=compute[0677-0684] Feature=broadwell weight=40 Sockets=2 CoresPerSocket=12 RealMemory=118000 State=UNKNOWN
NodeName=compute[0721-0744] Feature=broadwell weight=40 Sockets=2 CoresPerSocket=12 RealMemory=118000 State=UNKNOWN
NodeName=gpu[001-048] Feature=haswell,k80 weight=1030 gres=gpu:4 Sockets=2 CoresPerSocket=12 RealMemory=118000 State=UNKNOWN
NodeName=gpu[049-072] Feature=broadwell,k80 weight=1040 gres=gpu:4 Sockets=2 CoresPerSocket=14 CPUSpecList=12,13,26,27 RealMemory=128000 MemSpecLimit=10000 State=UNKNOWN
NodeName=gpu[073-077] Feature=broadwell,p100 weight=2040 gres=gpu:2 Sockets=2 CoresPerSocket=14 RealMemory=118000 State=UNKNOWN
NodeName=gpudev001 Feature=skylake,v100 weight=2040 gres=gpu:4 CPUs=64 Boards=1 SocketsPerBoard=4 CoresPerSocket=16 ThreadsPerCore=1 RealMemory=374000
NodeName=gpudev002 Feature=skylake,v100 weight=2040 gres=gpu:2 CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=86000
NodeName=compute[0685-0720] Feature=skylake weight=40 CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=86000 State=UNKNOWN
NodeName=compute[0745-0756] Feature=skylake weight=40 CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=86000 State=UNKNOWN
NodeName=compute[0781-0784] Feature=skylake weight=40 CPUs=24 Boards=1 SocketsPerBoard=2 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=86000 State=UNKNOWN
#
# NODE TYPES
# group nodes              cores ram     reserved ram/cpu nodes
# 1     compute[0001-0676] 24    128000  10000    4917    676
# 2     compute[0677-0684] 28    128000  10000    4215    8
# 3     compute[0685-0720] 24    96000   10000    3584    36
# 4     compute[0721-0744] 28    128000  10000    4215    24
# 5     compute[0745-0756] 24    96000   10000    3584    12
# 6     compute[0781-0784] 24    96000   10000    3584    12
# 7     bigmem[0001-0050]  24    1024032 10000    42252   50
# 8     gpu[001-048]       24    128000  10000    4917    48
# 9     gpu[049-072]       28    128000  10000    4215    24
# 10    gpu[073-077]       28    128000  10000    4215    5
# 11    gpudev001          64    385611  10000    5869    1
# 12    gpudev002          24    95307   10000    3555    1
#
# PARTITIONS
# name       groups   low/high? optimal RAM/CPU
# shared     1,4,5    high      4917 ~ 4900
# parallel   1,2,3,4  high      4917 ~ 4900
# lrgmem     7        exact     42252
# debug      1,2,8    high      4917 ~ 4900
# quickdebug 2        exact     4215 ~ 4200
# unlimited  1,7,8    high      4917 ~ 4900
# gpuk80     8,9      low       4215 ~ 4200
# gpup100    10       exact     4215 ~ 4200
# gpuv100    11,12    separate  3555 ~ 3550
#
#! NEED TO DISCUSS 2 items:
#! 1. since gpuv100 has two nodes with a big memory, I propose MaxMemPerCPU=5860 DefMemPerCPU=3550
#! 2. not sure what to do about largememory, which has: MaxMemPerCPU=21334 DefMemPerCPU=4388
#! 3. not sure what to do about scavenger MaxMemPerCPU=5120 DefMemPerCPU=4388
#
# DEFAULT MEMORY SETTINGS
# We set default and maximum to the same values.
# The default value is the *highest* ram/cpu for all types of nodes in each 
# partition. This ensures that we are using memory efficiently compared to the 
# case where we choose the lowest ram/cpu as the default. A user who submits to 
# shared without a memory request will get 4917MB by default. If they end up on 
# a Broadwell node, which has 28 cores, then the high default memory limit 
# dictates that e.g. a 12-core request will occupy half of the node, hence 
# undersubscribing 2 cores. The benefit is that the 232 nodes in in group 1 on 
# shared will *not* be wasting 32GB of memory [note that in this case the memory 
# benefit is ((128000-10000)/24.-(96000-10000)/24.)*24 = 32000MB]. This extra 
# memory benefit applies to parallel as well. 
# Note that SLURM respects the total memory request with the --mem flag so users
# who need a specific amount of memory and do not need the cores, can use that.
# There appears to be no need for users to reques a specific amount of memory 
# per core, since the cores and memory are both consumable resources.
#
partitionname=express DefaultTime=01:00:00 priority=3 MaxMemPerCPU=3583 DefMemPerCPU=3583 shared=no qos=express denyqos=scavenger state=up maxtime=0-12:0:0 nodes=compute[0717-0720],compute[0753-0756],compute[0781-784] TRESBillingWeights=CPU=1.0,Mem=0.192G default=yes
partitionname=skylake DefaultTime=01:00:00 priority=3 MaxMemPerCPU=3583 DefMemPerCPU=3583 shared=no qos=skylake denyqos=scavenger state=up maxtime=0-72:0:0 nodes=compute[0685-0716,0745-0752] TRESBillingWeights=CPU=1.0,Mem=0.192G default=no
partitionname=shared DefaultTime=01:00:00 priority=3 MaxMemPerCPU=4916 DefMemPerCPU=4916 shared=no qos=shared denyqos=scavenger state=up maxtime=3-0 nodes=compute[0001-0200],compute[0677-0684],compute[0721-0744] TRESBillingWeights=CPU=1.0,Mem=0.192G default=no
partitionname=parallel DefaultTime=01:00:00 priority=3 MaxMemPerCPU=4916 DefMemPerCPU=4916 shared=exclusive qos=parallel denyqos=scavenger state=up maxtime=3-0 nodes=compute[0201-0666] TRESBillingWeights=CPU=1.0,Mem=0.192G default=no
partitionname=gpuk80 DefaultTime=01:00:00 priority=24 MaxMemPerCPU=4916 DefMemPerCPU=4916 shared=no qos=gpuk80 denyqos=scavenger state=up maxtime=2-0 nodes=gpu[006-072] TRESBillingWeights=CPU=1,Mem=0.192G,gres/gpu=6.0
partitionname=gpup100 DefaultTime=01:00:00 priority=24 MaxMemPerCPU=4214 DefMemPerCPU=4214 shared=no qos=gpup100 allowqos=normal,rherna21,rherna21-gpup100 state=up maxtime=2-0 nodes=gpu[073-077] TRESBillingWeights=CPU=1,Mem=0.192G,gres/gpu=7.0
partitionname=gpuv100 DefaultTime=01:00:00 priority=24 MaxMemPerCPU=5843 DefMemPerCPU=3583 shared=no qos=gpuv100 allowqos=gpuv100 state=up maxtime=2-0 nodes=gpudev[001-002] TRESBillingWeights=CPU=1,Mem=0.192G,gres/gpu=6.0
partitionname=lrgmem DefaultTime=01:00:00 priority=24 MaxMemPerCPU=21125 DefMemPerCPU=21125 shared=no qos=lrgmem denyqos=scavenger state=up maxtime=3-0 nodes=bigmem[0006-0050] TRESBillingWeights="CPU=1,Mem=0.047619048G"
partitionname=debug DefaultTime=01:00:00 priority=3 MaxMemPerCPU=4916 DefMemPerCPU=4916 shared=no qos=debug denyqos=scavenger state=up maxtime=2:00:00 nodes=compute[0672-0676],gpu[001-003] TRESBillingWeights=CPU=1.0,Mem=0.192G maxnodes=2
partitionname=unlimited DefaultTime=01:00:00 priority=2 MaxMemPerCPU=21125 DefMemPerCPU=4916 shared=no denyqos=scavenger state=up maxtime=unlimited nodes=compute[0667-0671],bigmem[0001-0005],gpu[004-005] TRESBillingWeights=CPU=1.0,Mem=0.192G
partitionname=scavenger DefaultTime=01:00:00 priority=0 MaxMemPerCPU=4916 DefMemPerCPU=4916 shared=no allowqos=scavenger state=up maxtime=6:00:00 nodes=compute[0001-0671],compute[0679-0684],compute[0721-0744],bigmem[0001-0050] TRESBillingWeights=CPU=1.0,Mem=0.224G,gres/gpu=7.0
