# BLUE CRAB SOFTWARE STACK 
# Build and organize software for this cluster.

docs: |
  To use: `make use specs/cli_spack.yaml`
  To run: `make spack_tree specs/spack_tree.yaml gromacs_gcc6`
  To deploy on HPC: see spack_hpc_go.yaml for instructions.
caveats:
  blue-crab-ompi-notes: |
    Installing openmpi on bluecrab requires a special method.
    We build directly in production for access to slurm.
    Then rsync back to the testzone and concretize to confirm.
    This connects openmpi to external slurm.
  python-3-download-error: 
    Download errors with python for some reason (persistent).
    You must create a mirror and then add python to it.
      spack mirror create python@3.7.6
    Check for mirrors.yaml and beware polluting the user site.
    The mirror should reside in the spack tree and can be cleared later.
    This is the same procedure you would use for an air-gapped cluster.

# SETTINGS: spack source and environments locations 
spot: ./local/spack # config.json, spack
spot_envs: ./local/envs-spack # config.json, spack_envs

###
### CONFIGURATION
###

# template for spack.yaml used below
template_basic: 
  concretization: separately
  mirrors: {}
  repos: []
  upstreams: {}
  modules:
    enable: []
  definitions: []
  packages: {}
  config: 
    checksum: false
  specs: []
  view: false

bc-config: &bc-config-build
  # config for building
  config: &bc-config
    install_tree: /software/apps/spack/a02
    module_roots: 
      lmod: &module-spot /software/apps/spack/m02
    build_stage:
      - "$TMPDIR/$USER/spack-stage"
    misc_cache: /software/apps/spack/a02/cache_misc
  packages: &packages
    slurm:
      paths:
        slurm@17.11.12: /software/apps/slurm/17.11.12.1.marcc
      buildable: false
      version: []
      providers: {}
      modules: {}
      compiler: []
    openssl:
      paths: {openssl@1.0.2k: /usr}
      buildable: false

###
### SOFTWARE SPECIFICATION
###

openmpi-3_1_4: &openmpi-3_1_4 >-
    openmpi@3.1.4%gcc@7.4.0 +cuda +cxx_exceptions fabrics=verbs 
    +legacylaunchers+pmi schedulers=slurm ^slurm@17.11.12 ^cuda@9.2.88
    arch=linux-centos7-haswell

# base modules for gcc 7 must be cumulative for lmod
gcc-7-base: &gcc-7-base
  # include gcc for lmod
  - gcc@7.4.0 %gcc@4.8.5 arch=linux-centos7-haswell
  - gsl %gcc@7.4.0 arch=linux-centos7-haswell
  - cuda@9.2.88 %gcc@7.4.0 arch=linux-centos7-haswell
  - &cuda-10-2 cuda@10.2.89 %gcc@7.4.0 arch=linux-centos7-haswell
  #! segfaults: - &cuda-10-1 cuda@10.1.243 %gcc@7.4.0 arch=linux-centos7-haswell
  - &cuda-10-0 cuda@10.0.130 %gcc@7.4.0 arch=linux-centos7-haswell
  #! tf 2.1 with cuda 10.2 with cudnn requires a higher version than 7.5.1
  - &cudnn-7-5-1 cudnn@7.5.1.10-10.0-linux-x64 %gcc@7.4.0 arch=linux-centos7-haswell
  - &cudnn-7-6-5 cudnn@7.6.5.32-10.2-linux-x64 %gcc@7.4.0 arch=linux-centos7-haswell
  # note that we added ~libxml2 to the python3-gcc-7 spec to resolve collision /c2ullvq
  - &python3-gcc-7 python@3.7.6 %gcc@7.4.0 +optimizations +uuid ~libxml2 arch=linux-centos7-haswell
  - !chain
    - py-pip@19.3 %gcc@7.4.0 arch=linux-centos7-haswell
    - *python3-gcc-7
  - !chain
    - r@3.6.1 %gcc@7.4.0 arch=linux-centos7-haswell
    - *python3-gcc-7
  - !chain
    - >-
      py-tensorflow@2.1.0 %gcc@7.4.0 arch=linux-centos7-haswell
      +cuda cuda_arch=35 +numa ~mpi ~mkl ~tensorrt ~verbs
    #! tf 2.1.0 on rhel is pending bazel issue https://github.com/spack/spack/issues/14234
    #! tensorrt is disabled because it was asking for some extra flag
    #! extreme difficulty getting this to work
    - *cuda-10-2
    - *cudnn-7-6-5
    - *python3-gcc-7

deploy_history: |
  2020.01.23
    compiled openmpi in-place at the production location for access to slurm
    which is external. this was previously required for openmpi 4 but we also
    use this method for openmpi 3.1.4 to be sure it communications with slurm
  2020.01.29
    develop specs for py-tensorflow (tf), cuda, cudnn
    segfaults on cuda 10.1
    fail on tf 2.0.0 with cuda 10.0 and cudnn 7.5.1 due to bazel/jdk issue 
      https://github.com/spack/spack/issues/14234
  2020.01.30
    fail on tf 2.1.0 with cuda 10.2 and cudnn 7.6.5 (cudnn pinned to cuda) due to bazel/jdk

# modules for a language (python, R) must have deps added systematically
#   by looking at upstream py-* or r-* deps from a concretize and listing 
#   them below. they must also get an autoload flag under modules
#! dev note: this can be done quickly with a regex but we should automate

# py-tensorflow upstream python requirements
py-tensorflow-whitelist: &py-tensorflow-whitelist [py-tensorflow,
  py-absl-py, py-setuptools, py-six, py-astor, py-gast, 
  py-google-pasta, py-grpcio, py-cython, py-keras-applications, 
  py-keras-preprocessing, py-numpy, py-opt-einsum, py-protobuf, 
  py-scipy, py-pybind11, py-termcolor, py-wheel, py-wrapt]

# standard whitelist follows for lmod modulefiles
#! todo: automatically generate the whitelist for target packages e.g. py-pip for python
#! todo: automatically load some dependencies with python e.g. py-pip
whitelist-std: &whitelist-std !merge_lists
  #! this is broken!
  - [gcc, openmpi]
  # python requires pip and setuptools
  - [python, py-pip, py-setuptools]
  - *py-tensorflow-whitelist 

###
### MODULES
###

bc-config-modules: &bc-config-std
  # config with modules
  config: *bc-config
  packages: *packages
  modules:
    enable: 
      - lmod
    lmod:
      all:
        conflict: []
        environment:
          unset: []
        filter:
          environment_blacklist: []
        load: []
        #! hdf5 and netcdf are pending
        suffixes:
          hdf5+cxx: serial
          netcdf+parallel-netcdf: parallel
      blacklist:
      - '%gcc@4.8.5'
      blacklist_implicits: true
      core_compilers:
      - gcc@4.8.5
      hash_length: 0
      naming_scheme: '{name}/{version}/{compiler.name}/{compiler.version}'
      hierarchy:
      - compiler
      - mpi
      verbose: false
      # openmpi fork issue
      openmpi@3.4.1:
        environment:
          set:
            OMPI_MCA_mpi_warn_on_fork: '0'
          unset: []
        filter:
          environment_blacklist: []
        load: []
        conflict: []
      # whitelist controls what the user sees
      whitelist: *whitelist-std
      # modules with complex upstream dependencies are autoloaded
      py-tensorflow: {autoload: all}
      python: {autoload: all}
      r@3.6.1: {autoload: all}

### ENVIRONMENTS

bc-prelim:
  notes: |
    Initial test before running a compiler build.
    #! needs retested
  envs:
    - check_compiler: gcc@4.8.5
    - bootstrap: null
    - name: env-prelim
      via: template_basic
      #! needs retested after updates below
      mods: *bc-config-build
      specs:
        - zlib%gcc@4.8.5 arch=linux-centos7-haswell
        - cmake%gcc@4.8.5 arch=linux-centos7-haswell

bc-gcc-7:
  notes: Compile gcc 7 for Blue Crab.
  envs:
    - find_compilers: null
    - check_compiler: gcc@4.8.5
    - bootstrap: null
    - name: &env_gcc_7 env-gcc-7
      via: template_basic
      # build gcc without lmod refresh
      # lmod refresh occurs after we build packages below
      mods: *bc-config-build
      specs:
        - gcc@7.4.0 %gcc@4.8.5 arch=linux-centos7-haswell
    - find: gcc@7.4.0 
      name: *env_gcc_7

bc-gcc-7-base:
  notes: Blue Crab base apps built on gcc7.
  envs:
    - check_compiler: gcc@7.4.0
    - name: &env_gcc_7_base env-gcc-7-base
      via: template_basic
      mods: *bc-config-build
      specs: !merge_lists
        - *gcc-7-base

bc-gcc-7-openmpi-3:
  notes: Blue Crab openmpi 3 built on gcc 7.
  envs:
    - check_compiler: gcc@7.4.0
    - name: &env_gcc_7_openmpi_3 env-gcc-7-openmpi-3
      via: template_basic
      mods: *bc-config-build
      specs:
        - *openmpi-3_1_4 

bc-std:
  notes: |
    Standard module tree on the new "stack" including gcc/openmpi.
    This is the first point at which we can deploy lmod.
  envs:
    - check_compiler: gcc@7.4.0
    - name: &env_std env-std
      via: template_basic
      mods: *bc-config-std
      specs: !merge_lists
        - *gcc-7-base
        - - *openmpi-3_1_4 
    # specs must match mods modules for lmod refresh
    - lmod_refresh: null
      name: *env_std
    - lmod_hide_nested: *module-spot
    - lmod_hooks:
      - moduleroot: *module-spot
        modulefile: linux-centos7-x86_64/Core/original.lua
        contents: !strcat
          - 'execute{cmd="ml purge && '
          - "export MODULEPATH="
          - "/software/lmod/modulefiles/compiler_and_base:"
          - '/software/lmod/modulefiles/apps && ml restore",'
          - 'modeA={"load"}}'
