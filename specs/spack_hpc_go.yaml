docs: |
  GUIDE 1: QUICKSTART
    Primary workflow:
      0. Clone the factory.
      1. Customize a copy of `spack_hpc_go.yaml` (this file) for your account. 
           - Specify the factory location.
           - Choose a temporary directory on a fast filesystem.
           - Obtain a copy of the Singularity image matched to our base image.
           - Create a "decoy" testing space.
           - Create a copy of "the spec" at `specs/spack_hpc.yaml` for customization.
      2. Install a clean venv with only yaml.
           python=python3 make venv create file=specs/env_venv.txt
      3. Run the configure-test loop below, repeating as needed.
      4. Run the build loop when the concretize looks good.
      5. Push to production.
    Loop: configure-test, manual method
      0. Start an interact session
      1. In the factory location, run `source env.sh min`
      2. Edit your custom `spack_hpc_go_X.yaml` with a target `name` and `live: true`.
      3. Execute with: `make do specs/spack_hpc_go_X.yaml`
         Note that failures may leave a container-spack-hpc.sh that you should remove.
      4. If you have set `live: true` this deposits you in the decoy env.
           - You will be inside the container at `factory/local/spack`.
           - The program will run `spack -f concretize` inside an env (a subfolder).
           - The env folders are inside your pwd and correspond to the `name` you selected.
      5. You can repeat this loop in two ways:
           - Edit env `spack.yaml`, concretize in the container, and repeat until success.
           - Edit `specs/spack_hpc.yaml` and run the `make do` command above until success.
      6. When the environment looks good, make sure any local changes to the env at
           `spack.yaml` have been propagated back to `specs/spack_hpc.yaml`. 
           We recommend running `make do` with the finalized `specs/spack_hpc.yaml` 
           before continuing to the build loop.
    Loop: configure-test-build, automatic method
      > Note that in the "configure-test, manual method" above we are using `make do` and
        editing the "go" file (`spack_hpc_go.yaml`, this file or a custom copy) along with
        "the spec" (`spack_hpc.yaml`, or a custom copy). We set `live: true` to concretize 
        and then inspect things. If we set `live: false` it will run `spack install`.
        This procedure has been entirely automated according to the following instructions.
        These instructions allow you to run a command directly from the command line.
      1. Start by setting up this file, or a custom copy at `spack_hpc_go_X.yaml`.
      2. Run `bash specs/script-bc.sh` to see your options. There is an salloc and sbatch 
         command for running either interactive or batch jobs automatically.
         You can include the target `name` directly on the CLI to run a recipe in "the spec"
         file which is set in the `spack_hp_go.yaml` (this file or customized). The "go"
         file is also set directly on the CLI. Lastly, you can include the word "live" in 
         the command to trigger the interactive form. Note that this should only be used
         with salloc for obvious reasons (i.e. you will not get a terminal on sbatch).
      3. Edit "the spec" file to modify the spack deployment.
      4. Run a SLURM job to either inspect or build (see below for concrete examples).
      5. Repeat as necessary. Note that the interactive method is useful if you make a 
         mistake and need to remove something from the tree to avoid collisions.
    Examples:
      salloc --qos=gpuv100 -w gpudev002 --res image_tests \
        -c 12 --gres=gpu:1 srun --pty /bin/bash -c \
        'source specs/script-bc.sh bc-std live'
        # the above example runs the "bc-std" target interactively (with "live" and "salloc")
        # and uses, by default, this file (spack_hpc_go.yaml)
        #! DEV NOTE: the script should accept a custom "go" file
        # if you remove the "live" it will automatically build in the foreground
      sbatch specs/script-bc.sh bc-std
        # run the `bc-std` target automatically
        #! DEV NOTE: see above; we need a custom "go" file
    Pending development:
      Incoming features include automatic integration with the "big" (read: "original") stack.
      Ask Ryan for more details.

spot: !!python/object/apply:lib.spack.spack_hpc_singularity
  kwds:
    # this is "the spec" and contains ALL of the spack information
    # you should create custom copies of both specs/spack_hpc.yaml 
    #   and specs/spack_hpc_go.yaml and make sure all paths match
    spec: specs/spack_hpc.yaml
    # make a custom temporary directory for staging spack builds
    tmpdir: /exec/rbradley/stage
    # select a name which must be a root-level key in the spec (above)
    # this name typically corresponds to a spack environment
    # the root-level key in the spec file has a special syntax
    name: bc-gcc-7-base #bc-std # null to ask for use select in interact
    # make sure you use an interact session and set live for the configure-test loop
    live: true 
    # use the alternative proot method for builds against openmpi
    # note that proot only applies to build and not live
    #! the proot method needs clarified
    proot: true 
    mounts: 
      # create a custom decoy location where spack will build new packages
      # singularity maps the decoy location to the production location in the container
      - host: /exec/rbradley/buildsite/testzone/software/apps/spack
        local: /software/apps/spack
      # make sure the path below matches the factory location
      - host: /exec/rbradley/buildsite/factory
      - host: /software/apps/slurm/17.11.12.1.marcc
      - host: /software/centos7 # added for openmpi special build
      # openssl
      - host: /usr/bin/openssl
        local: /usr/bin/openssl
      - host: /etc/ssl
        local: /etc/ssl
    # obtain a copy of the singularity image matched to the base image
    image: /exec/rbradley/buildsite/mimic/mimic.simg
